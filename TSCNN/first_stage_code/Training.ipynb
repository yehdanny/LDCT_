{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6811e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ygz08\\AppData\\Local\\Temp\\ipykernel_3428\\2910766302.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(LOAD_TRAIN_PATH)\n",
      "C:\\Users\\ygz08\\AppData\\Local\\Temp\\ipykernel_3428\\2910766302.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data2 = torch.load(LOAD_TRAIN_PATH2)\n",
      "C:\\Users\\ygz08\\AppData\\Local\\Temp\\ipykernel_3428\\2910766302.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_data = torch.load(LOAD_VAL_PATH)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "LOAD_TRAIN_PATH2 = r\"D:\\Daniel\\for_git\\LDCT_git\\TSCNN\\first_stage_code\\TrainVal\\TRAIN2.pth\"\n",
    "LOAD_TRAIN_PATH = r\"D:\\Daniel\\for_git\\LDCT_git\\TSCNN\\first_stage_code\\TrainVal\\TRAIN3.pth\"\n",
    "\n",
    "LOAD_VAL_PATH = r\"D:\\Daniel\\for_git\\LDCT_git\\TSCNN\\first_stage_code\\TrainVal\\VAL3.pth\"\n",
    "\n",
    "train_data = torch.load(LOAD_TRAIN_PATH)\n",
    "train_data2 = torch.load(LOAD_TRAIN_PATH2)\n",
    "val_data = torch.load(LOAD_VAL_PATH)\n",
    "\n",
    "load_train_patches = train_data['patches'] #(N_total, 3, 64, 64) #0~255\n",
    "load_train_labels = train_data['masks'] #(N_total, 1, 64, 64) #0 or 1\n",
    "\n",
    "\n",
    "load_train_patches2 = train_data2['patches'] #(N_total, 3, 64, 64) #0~255\n",
    "load_train_labels2 = train_data2['masks'] #(N_total, 1, 64, 64) #0 or 1\n",
    "\n",
    "load_val_patches = val_data['patches'] #(N_total, 3, 64, 64) #0~255\n",
    "load_val_labels = val_data['masks'] #(N_total, 1, 64, 64) #0 or 1\n",
    "\n",
    "load_train_patches = torch.cat([load_train_patches, load_train_patches2], dim=0)\n",
    "load_train_labels = torch.cat([load_train_labels, load_train_labels2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd2342a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train æœ‰ 2421 å€‹æ¨£æœ¬ï¼ŒVal æœ‰ 330 å€‹æ¨£æœ¬ã€‚\n",
      "âœ… æ‰€æœ‰ Tensor çš„ç¶­åº¦èˆ‡å‹æ…‹æª¢æŸ¥é€šéï¼\n"
     ]
    }
   ],
   "source": [
    "if train_data and val_data :\n",
    "    # æª¢æŸ¥è¨“ç·´ Patches ç¶­åº¦\n",
    "    # ç¢ºä¿æ˜¯å››å€‹ç¶­åº¦ï¼Œä¸” Channel=3, H=64, W=64ã€‚N_total ä¸å›ºå®šï¼Œç”¨ -1 æª¢æŸ¥\n",
    "    assert load_train_patches.dim() == 4, \\\n",
    "        f\"è¨“ç·´ Patches ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_train_patches.dim()} ç¶­.\"\n",
    "    assert load_train_patches.shape[1:] == (3, 64, 64), \\\n",
    "        f\"è¨“ç·´ Patches Shape éŒ¯èª¤! é æœŸ (N, 3, 64, 64), å¯¦éš› {load_train_patches.shape}.\"\n",
    "\n",
    "    # æª¢æŸ¥è¨“ç·´ Labels (Masks) ç¶­åº¦\n",
    "    # ç¢ºä¿æ˜¯å››å€‹ç¶­åº¦ï¼Œä¸” Channel=1, H=64, W=64\n",
    "    assert load_train_labels.dim() == 4, \\\n",
    "        f\"è¨“ç·´ Labels ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_train_labels.dim()} ç¶­.\"\n",
    "    assert load_train_labels.shape[1:] == (1, 64, 64), \\\n",
    "        f\"è¨“ç·´ Labels Shape éŒ¯èª¤! é æœŸ (N, 1, 64, 64), å¯¦éš› {load_train_labels.shape}.\"\n",
    "\n",
    "    # æª¢æŸ¥è¨“ç·´è³‡æ–™çš„æ‰¹æ¬¡å¤§å°æ˜¯å¦ä¸€è‡´\n",
    "    assert load_train_patches.shape[0] == load_train_labels.shape[0], \\\n",
    "        f\"è¨“ç·´è³‡æ–™æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´! Patches: {load_train_patches.shape[0]}, Labels: {load_train_labels.shape[0]}.\"\n",
    "        \n",
    "    # å»ºè­°çš„è³‡æ–™å‹æ…‹æª¢æŸ¥\n",
    "    # Patches (è¼¸å…¥å½±åƒ) é€šå¸¸æ˜¯ float\n",
    "    # Labels (é®ç½©) é€šå¸¸æ˜¯ float æˆ– long (å¦‚æœæ˜¯é¡åˆ¥ç´¢å¼•)\n",
    "    assert load_train_patches.dtype == torch.float32, \\\n",
    "        f\"è¨“ç·´ Patches è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32, å¯¦éš› {load_train_patches.dtype}.\"\n",
    "    assert load_train_labels.dtype in [torch.float32, torch.long], \\\n",
    "        f\"è¨“ç·´ Labels è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32 æˆ– torch.long, å¯¦éš› {load_train_labels.dtype}.\"\n",
    "\n",
    "    # --- ğŸ¯ æª¢æŸ¥é©—è­‰è³‡æ–™ (VAL Data Assertions) ---\n",
    "\n",
    "    # æª¢æŸ¥é©—è­‰ Patches ç¶­åº¦\n",
    "    assert load_val_patches.dim() == 4, \\\n",
    "        f\"é©—è­‰ Patches ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_val_patches.dim()} ç¶­.\"\n",
    "    assert load_val_patches.shape[1:] == (3, 64, 64), \\\n",
    "        f\"é©—è­‰ Patches Shape éŒ¯èª¤! é æœŸ (N, 3, 64, 64), å¯¦éš› {load_val_patches.shape}.\"\n",
    "\n",
    "    # æª¢æŸ¥é©—è­‰ Labels (Masks) ç¶­åº¦\n",
    "    assert load_val_labels.dim() == 4, \\\n",
    "        f\"é©—è­‰ Labels ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_val_labels.dim()} ç¶­.\"\n",
    "    assert load_val_labels.shape[1:] == (1, 64, 64), \\\n",
    "        f\"é©—è­‰ Labels Shape éŒ¯èª¤! é æœŸ (N, 1, 64, 64), å¯¦éš› {load_val_labels.shape}.\"\n",
    "        \n",
    "    # æª¢æŸ¥é©—è­‰è³‡æ–™çš„æ‰¹æ¬¡å¤§å°æ˜¯å¦ä¸€è‡´\n",
    "    assert load_val_patches.shape[0] == load_val_labels.shape[0], \\\n",
    "        f\"é©—è­‰è³‡æ–™æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´! Patches: {load_val_patches.shape[0]}, Labels: {load_val_labels.shape[0]}.\"\n",
    "\n",
    "    # å»ºè­°çš„è³‡æ–™å‹æ…‹æª¢æŸ¥\n",
    "    assert load_val_patches.dtype == torch.float32, \\\n",
    "        f\"é©—è­‰ Patches è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32, å¯¦éš› {load_val_patches.dtype}.\"\n",
    "    assert load_val_labels.dtype in [torch.float32, torch.long], \\\n",
    "        f\"é©—è­‰ Labels è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32 æˆ– torch.long, å¯¦éš› {load_val_labels.dtype}.\"\n",
    "\n",
    "    print(\"âœ… Train æœ‰ {} å€‹æ¨£æœ¬ï¼ŒVal æœ‰ {} å€‹æ¨£æœ¬ã€‚\".format(load_train_patches.shape[0], load_val_patches.shape[0]))\n",
    "    print(\"âœ… æ‰€æœ‰ Tensor çš„ç¶­åº¦èˆ‡å‹æ…‹æª¢æŸ¥é€šéï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea0012b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´è³‡æ–™é›†å¤§å°: 2421\n",
      "è¨“ç·´æ‰¹æ¬¡æ•¸é‡: 76\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1. ç¢ºä¿æ•¸æ“šé¡å‹å’Œç¯„åœæ­£ç¢º\n",
    "# patches (å½±åƒ) é€šå¸¸éœ€è¦æ˜¯ float é¡å‹ï¼Œlabels (æ©ç¢¼) ä¹Ÿæ˜¯ float (å› ç‚º sigmoid è¼¸å‡º)\n",
    "# ä¸¦ä¸”å°‡ 0~255 çš„ patches æ­¸ä¸€åŒ–åˆ° 0~1 ç¯„åœ (å¦‚æœå°šæœªæ­¸ä¸€åŒ–)\n",
    "\n",
    "# å‡è¨­åŸå§‹ patches æ˜¯ 0~255 çš„ uint8 æˆ– float é¡å‹\n",
    "# é€™è£¡å°‡å…¶è½‰ç‚º float ä¸¦æ­¸ä¸€åŒ– (å¦‚æœæ‚¨çš„é è™•ç†é‚„æ²’åšé€™æ­¥)\n",
    "# å¦‚æœæ‚¨çš„æ•¸æ“šå·²ç¶“æ˜¯æ­¸ä¸€åŒ–çš„ floatï¼Œå¯ä»¥è·³éé€™ä¸€æ­¥çš„é™¤ä»¥ 255\n",
    "load_train_patches = load_train_patches.float() / 255.0\n",
    "load_val_patches = load_val_patches.float() / 255.0\n",
    "\n",
    "load_train_labels = load_train_labels.float()\n",
    "load_val_labels = load_val_labels.float()\n",
    "\n",
    "# 2. å‰µå»º TensorDataset\n",
    "train_dataset = TensorDataset(load_train_patches, load_train_labels)\n",
    "val_dataset = TensorDataset(load_val_patches, load_val_labels)\n",
    "\n",
    "# 3. å‰µå»º DataLoader\n",
    "BATCH_SIZE = 32 # å¯ä»¥æ ¹æ“šä½ çš„ GPU è¨˜æ†¶é«”èª¿æ•´\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"è¨“ç·´è³‡æ–™é›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"è¨“ç·´æ‰¹æ¬¡æ•¸é‡: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68701f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 1. è«–æ–‡å°ˆç”¨ Loss å‡½æ•¸ (Dice å½¢å¼) ---\n",
    "# éµå®ˆè«–æ–‡å…¬å¼: Dice_Loss = 1 - (2 * V(Gt âˆ© Seg) + Î·) / (V(Gt) + V(Seg) + Î·)\n",
    "def paper_dice_loss(pred, target, eta=1e-6): # ä½¿ç”¨ eta æ›¿æ› smoothï¼Œå¼·èª¿å°æ‡‰è«–æ–‡ä¸­çš„ Î·\n",
    "    \"\"\"\n",
    "    è¨ˆç®—è«–æ–‡ä¸­æ‰€ç¤ºçš„ Dice å½¢å¼çš„æå¤±å‡½æ•¸ (Loss Function)ã€‚\n",
    "    \n",
    "    Args:\n",
    "        pred (Tensor): æ¨¡å‹çš„é æ¸¬æ©Ÿç‡ (0-1)ã€‚\n",
    "        target (Tensor): çœŸå¯¦åˆ†å‰²æ©ç¢¼ (0/1)ã€‚\n",
    "        eta (float): è«–æ–‡ä¸­é˜²æ­¢åˆ†æ¯ç‚ºé›¶çš„éé›¶åƒæ•¸ (å¹³æ»‘é …)ã€‚\n",
    "    \"\"\"\n",
    "    # å±•å¹³å¼µé‡ (V(X) ç›¸ç•¶æ–¼å…ƒç´ ç¸½å’Œ)\n",
    "    pred_flat = pred.contiguous().view(-1)\n",
    "    target_flat = target.contiguous().view(-1)\n",
    "    \n",
    "    # V(Gt âˆ© Seg) = äº¤é›†\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    \n",
    "    # V(Gt) + V(Seg) = ç¸½å’Œ\n",
    "    union = pred_flat.sum() + target_flat.sum()\n",
    "    \n",
    "    # è¨ˆç®—è«–æ–‡ä¸­çš„ Dice å…¬å¼ (æ³¨æ„é€™è£¡ pred å’Œ target éƒ½æ˜¯ soft probability/float)\n",
    "    dice_numerator = 2. * intersection + eta\n",
    "    dice_denominator = union + eta\n",
    "    \n",
    "    dice_score_term = dice_numerator / dice_denominator\n",
    "    \n",
    "    # è«–æ–‡æå¤±å‡½æ•¸ = 1 - Dice Score Term\n",
    "    loss = 1 - dice_score_term\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# --- 2. ç§»é™¤ paper_dice_loss (ä¸å†ä½¿ç”¨ BCE) ---\n",
    "# åœ¨è¨“ç·´è¿´åœˆä¸­ï¼Œç›´æ¥ä½¿ç”¨ loss = paper_dice_loss(outputs, masks)\n",
    "\n",
    "# --- 3. è©•ä¼°æŒ‡æ¨™ (Dice Score) ä¿ç•™ ---\n",
    "# è©•ä¼°æ™‚ä»ç„¶ä½¿ç”¨ Dice Score (é€šå¸¸æ˜¯äºŒå€¼åŒ–å¾Œçš„çµæœ)\n",
    "def dice_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"è¨ˆç®— Dice ä¿‚æ•¸ (Dice Coefficient)ï¼Œç”¨æ–¼æ€§èƒ½è©•ä¼° (é€šå¸¸ä½¿ç”¨äºŒå€¼åŒ–)ã€‚\"\"\"\n",
    "    # ç¢ºä¿è¼¸å…¥æ˜¯äºŒå…ƒ (0 æˆ– 1)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    pred_flat = pred.contiguous().view(-1)\n",
    "    target_flat = target.contiguous().view(-1)\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    \n",
    "    return dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07933311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 1. åŸºç¤å–®å…ƒ: DenseBlock (DB) ---\n",
    "class DenseBlock(nn.Module):\n",
    "    # growth_rate å°±æ˜¯è«–æ–‡ä¸­çš„ n (8, 16, 32, ...)\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate=0.3):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        \n",
    "        # 1. ç¬¬ä¸€å€‹ Conv: BN -> ReLU -> Conv\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels), # æœŸæœ›è¼¸å…¥é€šé“æ•¸\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False),\n",
    "            nn.Dropout(dropout_rate) \n",
    "        )\n",
    "        \n",
    "        # 2. ç¬¬äºŒå€‹ Conv: BN -> ReLU -> Conv\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(growth_rate), # è¼¸å…¥ä¾†è‡ªä¸Šä¸€å€‹ conv1 çš„è¼¸å‡º\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(growth_rate, growth_rate, kernel_size=3, padding=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x) \n",
    "        x2 = self.conv2(x1)\n",
    "        \n",
    "        # è¼¸å‡ºæ˜¯ (è¼¸å…¥ x) å’Œ (æœ€çµ‚å·ç©è¼¸å‡º x2) çš„ä¸²è¯\n",
    "        return torch.cat([x, x2], dim=1) \n",
    "\n",
    "\n",
    "# --- 2. å †ç–Šå–®å…ƒ: Residual Dense Block (RDB) ---\n",
    "class ResDenseBlock(nn.Module):\n",
    "    # RDB çš„ in_channels å¿…é ˆç­‰æ–¼å®ƒçš„è¼¸å‡ºé€šé“æ•¸ (ç‚ºæ»¿è¶³ U-Net çš„è·³èºé€£æ¥è¦æ±‚)\n",
    "    def __init__(self, in_channels, n_feature, n_layers=6, dropout_rate=0.3):\n",
    "        super(ResDenseBlock, self).__init__()\n",
    "        \n",
    "        self.n_feature = n_feature \n",
    "        layers = []\n",
    "        current_channels = in_channels # å¾ RDB çš„è¼¸å…¥é€šé“é–‹å§‹\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            # æ¯å±¤ DB çš„è¼¸å…¥æ˜¯å‰ä¸€å±¤çš„ç¸½è¼¸å‡º\n",
    "            db = DenseBlock(current_channels, self.n_feature, dropout_rate)\n",
    "            layers.append(db)\n",
    "            # é€šé“æ•¸å¢åŠ  n_feature\n",
    "            current_channels += self.n_feature \n",
    "\n",
    "        self.dense_blocks = nn.Sequential(*layers)\n",
    "        \n",
    "        # 1x1 å·ç©éæ¸¡å±¤ (Transition Layer): \n",
    "        # å°‡ç¸½è¼¸å‡ºé€šé“æ•¸ (current_channels) è½‰æ›å› RDB æœŸæœ›çš„è¼¸å‡ºé€šé“ (in_channels)ï¼Œä»¥é€²è¡Œæ®˜å·®é€£æ¥\n",
    "        # é€™æ˜¯æ»¿è¶³ RDB è¼¸å‡ºé€šé“ = è¼¸å…¥é€šé“çš„é—œéµï¼\n",
    "        self.transition_conv = nn.Conv2d(current_channels, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_output = self.dense_blocks(x)\n",
    "        processed_output = self.transition_conv(dense_output)\n",
    "        \n",
    "        # æ®˜å·®é€£æ¥ï¼šè¼¸å‡º = å£“ç¸®å¾Œçš„å¯†é›†è¼¸å‡º + åŸå§‹è¼¸å…¥\n",
    "        return processed_output + x\n",
    "\n",
    "\n",
    "# --- 3. æ•´é«”æ¶æ§‹ï¼šResDense UNet (ä¿®æ­£ç‰ˆ 3) ---\n",
    "class ResDenseUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(ResDenseUNet, self).__init__()\n",
    "        \n",
    "        # è«–æ–‡åƒæ•¸ n: [8, 16, 32, 32, 16, 8]\n",
    "        n_features = [8, 16, 32, 32, 16, 8] \n",
    "        \n",
    "        # --- é€šç”¨å±¤ ---\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        def create_upconv(in_c, out_c):\n",
    "            # ä½¿ç”¨ output_padding=1 ç¢ºä¿æ­£ç¢ºçš„å°ºå¯¸åŠ å€ (ä¾‹å¦‚ 8->16, 16->32)\n",
    "            return nn.ConvTranspose2d(in_c, out_c, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # --- Encoder (ç¸®æ¸›è·¯å¾‘) ---\n",
    "        \n",
    "        # 1. åˆå§‹ Conv (3 -> 8)\n",
    "        self.init_conv = nn.Conv2d(in_channels, n_features[0], kernel_size=1) \n",
    "        self.rdb1 = ResDenseBlock(in_channels=n_features[0], n_feature=n_features[0]) # Output C=8\n",
    "\n",
    "        # 2. é€šé“èª¿æ•´ (Max Pool å¾Œ): 8 -> 16\n",
    "        self.trans_conv1 = nn.Conv2d(n_features[0], n_features[1], kernel_size=1) \n",
    "        self.rdb2 = ResDenseBlock(in_channels=n_features[1], n_feature=n_features[1]) # Output C=16\n",
    "\n",
    "        # 3. é€šé“èª¿æ•´ (Max Pool å¾Œ): 16 -> 32\n",
    "        self.trans_conv2 = nn.Conv2d(n_features[1], n_features[2], kernel_size=1) \n",
    "        self.rdb3 = ResDenseBlock(in_channels=n_features[2], n_feature=n_features[2]) # Output C=32 (Bottleneck Input)\n",
    "\n",
    "        # --- Decoder (æ“´å¢è·¯å¾‘) ---\n",
    "        \n",
    "        # 4. Stage 4: 8x8 -> 16x16\n",
    "        # upconv4 è¼¸å…¥=Bottleneck è¼¸å‡º(32), è¼¸å‡º=n_features[3]=32 (ç”¨æ–¼ Concatenate)\n",
    "        self.upconv4 = create_upconv(n_features[2], n_features[3]) \n",
    "        # rdb4 è¼¸å…¥: upconv(32) + skip(32) = 64\n",
    "        self.rdb4 = ResDenseBlock(in_channels=n_features[3] + n_features[2], n_feature=n_features[3]) # Output C=64\n",
    "        \n",
    "        # 5. Stage 5: 16x16 -> 32x32\n",
    "        # upconv5 è¼¸å…¥=rdb4_out(64), è¼¸å‡º=n_features[4]=16\n",
    "        self.upconv5 = create_upconv(n_features[3] + n_features[2], n_features[4])\n",
    "        # rdb5 è¼¸å…¥: upconv(16) + skip(16) = 32\n",
    "        self.rdb5 = ResDenseBlock(in_channels=n_features[4] + n_features[1], n_feature=n_features[4]) # Output C=32\n",
    "\n",
    "        # 6. Stage 6: 32x32 -> 64x64\n",
    "        # upconv6 è¼¸å…¥=rdb5_out(32), è¼¸å‡º=n_features[5]=8\n",
    "        self.upconv6 = create_upconv(n_features[4] + n_features[1], n_features[5])\n",
    "        # rdb6 è¼¸å…¥: upconv(8) + skip(8) = 16\n",
    "        self.rdb6 = ResDenseBlock(in_channels=n_features[5] + n_features[0], n_feature=n_features[5]) # Output C=16\n",
    "\n",
    "        # --- æœ€çµ‚è¼¸å‡ºå±¤ (16 -> 1) ---\n",
    "        self.final_conv = nn.Conv2d(n_features[5] + n_features[0], out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # --- æ‰“å° Shape ä»¥ä¾›æª¢æŸ¥ ---\n",
    "        # print(f\"Input: {x.shape}\")\n",
    "        \n",
    "        # 1. Initial Conv & RDB1\n",
    "        x = self.init_conv(x) \n",
    "        rdb1_out = self.rdb1(x) # Skip 1 (C=8, 64x64)\n",
    "        \n",
    "        # 2. Stage 2\n",
    "        p1 = self.pool(rdb1_out) \n",
    "        t1 = self.trans_conv1(p1) # C: 8 -> 16\n",
    "        rdb2_out = self.rdb2(t1) # Skip 2 (C=16, 32x32)\n",
    "        \n",
    "        # 3. Stage 3\n",
    "        p2 = self.pool(rdb2_out)\n",
    "        t2 = self.trans_conv2(p2) # C: 16 -> 32\n",
    "        rdb3_out = self.rdb3(t2) # Skip 3 (C=32, 16x16)\n",
    "        \n",
    "        # 4. Bottleneck\n",
    "        p3 = self.pool(rdb3_out) # Bottleneck (C=32, 8x8)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        \n",
    "        # 5. Stage 4: 8x8 -> 16x16\n",
    "        up4 = self.upconv4(p3) # C=32\n",
    "        cat4 = torch.cat([up4, rdb3_out], dim=1) # C=64\n",
    "        rdb4_out = self.rdb4(cat4) # C=64\n",
    "        \n",
    "        # 6. Stage 5: 16x16 -> 32x32\n",
    "        up5 = self.upconv5(rdb4_out) # C=16\n",
    "        cat5 = torch.cat([up5, rdb2_out], dim=1) # C=32\n",
    "        rdb5_out = self.rdb5(cat5) # C=32\n",
    "\n",
    "        # 7. Stage 6: 32x32 -> 64x64\n",
    "        up6 = self.upconv6(rdb5_out) # C=8\n",
    "        cat6 = torch.cat([up6, rdb1_out], dim=1) # C=16\n",
    "        rdb6_out = self.rdb6(cat6) # C=16\n",
    "\n",
    "        # 8. Output\n",
    "        return self.sigmoid(self.final_conv(rdb6_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0769b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "\n",
    "# éµå®ˆè«–æ–‡è¨­ç½®çš„åƒæ•¸\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 15 # ç¸½è¨“ç·´ä»£æ•¸\n",
    "INITIAL_LR = 0.0001\n",
    "FINE_TUNE_LR = 0.00001 # æœ€ä½å­¸ç¿’ç‡ (min_lr)ï¼Œæ¨¡æ“¬ Fine-tuning éšæ®µ\n",
    "\n",
    "# å‡è¨­ paper_dice_loss å’Œ dice_score å·²ç¶“è¢«å®šç¾©\n",
    "# å‡è¨­ model, train_loader, val_loader, optimizer, device å·²ç¶“è¢«åˆå§‹åŒ–\n",
    "\n",
    "def train_model_optimized_v3(model, train_loader, val_loader, optimizer, num_epochs, device, keep_train_path=None):\n",
    "    \"\"\"\n",
    "    å„ªåŒ–å¾Œçš„æ¨¡å‹è¨“ç·´å‡½æ•¸ï¼Œæ”¯æŒå­¸ç¿’ç‡èª¿åº¦ã€æ—©åœå’Œæ¨¡å‹æ¬Šé‡åŠ è¼‰ç¹¼çºŒè¨“ç·´ã€‚\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): å¾…è¨“ç·´çš„æ¨¡å‹ã€‚\n",
    "        train_loader (DataLoader): è¨“ç·´è³‡æ–™è¼‰å…¥å™¨ã€‚\n",
    "        val_loader (DataLoader): é©—è­‰è³‡æ–™è¼‰å…¥å™¨ã€‚\n",
    "        optimizer (optim.Optimizer): å„ªåŒ–å™¨ã€‚\n",
    "        num_epochs (int): ç¸½è¨“ç·´ä»£æ•¸ã€‚\n",
    "        device (torch.device): è¨“ç·´è¨­å‚™ (å¦‚ 'cuda' æˆ– 'cpu')ã€‚\n",
    "        keep_train_path (str, optional): ç¹¼çºŒè¨“ç·´æ™‚åŠ è¼‰çš„æ¨¡å‹æ¬Šé‡è·¯å¾‘ã€‚é è¨­ç‚º Noneã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. æª¢æŸ¥ä¸¦åŠ è¼‰æ¨¡å‹æ¬Šé‡ ---\n",
    "    if keep_train_path:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(keep_train_path, map_location=device))\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=INITIAL_LR)\n",
    "            print(f\"âœ… æˆåŠŸåŠ è¼‰æ¨¡å‹æ¬Šé‡ï¼Œå¾ä»¥ä¸‹è·¯å¾‘ç¹¼çºŒè¨“ç·´: {keep_train_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ éŒ¯èª¤: æœªæ‰¾åˆ°æ¨¡å‹æ¬Šé‡æª”æ¡ˆæ–¼è·¯å¾‘: {keep_train_path}ã€‚å°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ åŠ è¼‰æ¨¡å‹æ¬Šé‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚å°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚\")\n",
    "    \n",
    "    # --- 2. åˆå§‹åŒ–å­¸ç¿’ç‡èª¿åº¦å™¨ ---\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=0.1, \n",
    "        patience=5, \n",
    "        verbose=True, \n",
    "        min_lr=FINE_TUNE_LR \n",
    "    )\n",
    "    \n",
    "    # ç¨ç«‹çš„æ—©åœè¨ˆæ•¸å™¨ (ç”¨æ–¼å®Œå…¨åœæ­¢è¨“ç·´)\n",
    "    early_stop_counter = 0\n",
    "    early_stop_patience = 10 # å¢åŠ ä¸€å€‹æ›´å¤§çš„å¯¬é™æœŸï¼Œç”¨æ–¼å®Œå…¨åœæ­¢\n",
    "    \n",
    "    # æ³¨æ„ï¼šå¦‚æœå¾ Checkpoint ç¹¼çºŒè¨“ç·´ï¼Œé€™è£¡çš„ best_dice æ‡‰è©²å¾ Checkpoint ä¸­è®€å–ï¼Œ\n",
    "    # ä½†ç‚ºç°¡åŒ–ï¼Œé€™è£¡åˆå§‹åŒ–ç‚º 0.0ï¼Œå‡è¨­ç›®æ¨™æ˜¯æ‰¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚\n",
    "    best_dice = 0.0 \n",
    "    \n",
    "    print(f\"è¨“ç·´é–‹å§‹ï¼šBatch Size={BATCH_SIZE}, åˆå§‹/ç•¶å‰ LR={optimizer.param_groups[0]['lr']:.7f}\")\n",
    "    \n",
    "    # --- 3. è¨“ç·´è¿´åœˆ ---\n",
    "    for epoch in range(num_epochs):\n",
    "        # ------------------ è¨“ç·´éšæ®µ ------------------\n",
    "        model.train() \n",
    "        train_loss_sum = 0\n",
    "        \n",
    "        for inputs, masks in train_loader:\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # ç¢ºä¿ paper_dice_loss å’Œ dice_score å·²è¢«å®šç¾©\n",
    "            loss = paper_dice_loss(outputs, masks) \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += loss.item() * inputs.size(0)\n",
    "            \n",
    "        train_loss_avg = train_loss_sum / len(train_loader.dataset)\n",
    "        \n",
    "        # ------------------ é©—è­‰éšæ®µ ------------------\n",
    "        model.eval() \n",
    "        val_loss_sum = 0\n",
    "        val_dice_sum = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, masks in val_loader:\n",
    "                inputs, masks = inputs.to(device), masks.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = paper_dice_loss(outputs, masks)\n",
    "                val_loss_sum += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # è¨ˆç®— Dice ä¿‚æ•¸\n",
    "                val_dice_sum += dice_score(outputs, masks) * inputs.size(0)\n",
    "                \n",
    "        val_loss_avg = val_loss_sum / len(val_loader.dataset)\n",
    "        val_dice_avg = val_dice_sum / len(val_loader.dataset)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} (LR: {current_lr:.7f}): Train Loss={train_loss_avg:.4f}, \"\n",
    "              f\"Val Loss={val_loss_avg:.4f}, Val DICE Score={val_dice_avg:.4f}\")\n",
    "        \n",
    "        # ------------------ å­¸ç¿’ç‡èª¿åº¦ ------------------\n",
    "        scheduler.step(val_dice_avg)\n",
    "\n",
    "        # ------------------ æ¨¡å‹ä¿å­˜èˆ‡æ—©åœé‚è¼¯ ------------------\n",
    "        \n",
    "        if val_dice_avg > best_dice:\n",
    "            best_dice = val_dice_avg\n",
    "            early_stop_counter = 0 # æ€§èƒ½æå‡ï¼Œé‡ç½®æ—©åœè¨ˆæ•¸å™¨\n",
    "            # ğŸ’¡ æœ€ä½³æ¨¡å‹æ¬Šé‡ä¿å­˜ (Checkpointing)\n",
    "            torch.save(model.state_dict(), 'pth/best_stage1_unet_temp.pth')\n",
    "            print(\"ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\")\n",
    "        else:\n",
    "            # æ€§èƒ½æœªæå‡\n",
    "            early_stop_counter += 1\n",
    "            print(f\"æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ {early_stop_counter} epochsã€‚\")\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦è§¸ç™¼å®Œå…¨åœæ­¢\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                # æª¢æŸ¥ç•¶å‰ LR æ˜¯å¦å·²ç¶“æ˜¯æœ€ä½ Fine-tuning LR\n",
    "                if current_lr <= FINE_TUNE_LR * 1.001: \n",
    "                    print(f\"ğŸš¨ æ—©åœè§¸ç™¼ï¼é©—è­‰æ€§èƒ½é€£çºŒ {early_stop_patience} epochs æœªæå‡ï¼Œä¸”å·²é”æœ€ä½ LR ({FINE_TUNE_LR})ã€‚è¨“ç·´å®Œå…¨åœæ­¢ã€‚\")\n",
    "                    return\n",
    "                else:\n",
    "                    print(f\"âš ï¸ æ—©åœè¨ˆæ•¸ ({early_stop_counter}) è¶…éæ—©æœŸå¯¬é™æœŸï¼Œä½† LR æ‡‰åœ¨ {scheduler.patience} epochs å¾Œèª¿æ•´ã€‚\")\n",
    "\n",
    "\n",
    "    print(\"ç¸½è¨“ç·´ä»£æ•¸å·²å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a53e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´é–‹å§‹ï¼šBatch Size=64, åˆå§‹/ç•¶å‰ LR=0.0001000\n",
      "Epoch 1/15 (LR: 0.0001000): Train Loss=0.7461, Val Loss=0.6517, Val DICE Score=0.4347\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 2/15 (LR: 0.0001000): Train Loss=0.5374, Val Loss=0.5604, Val DICE Score=0.4714\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 3/15 (LR: 0.0001000): Train Loss=0.3423, Val Loss=0.5546, Val DICE Score=0.4510\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 1 epochsã€‚\n",
      "Epoch 4/15 (LR: 0.0001000): Train Loss=0.2751, Val Loss=0.4545, Val DICE Score=0.5470\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 5/15 (LR: 0.0001000): Train Loss=0.2525, Val Loss=0.3875, Val DICE Score=0.6143\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 6/15 (LR: 0.0001000): Train Loss=0.2772, Val Loss=0.2887, Val DICE Score=0.7135\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 7/15 (LR: 0.0001000): Train Loss=0.2533, Val Loss=0.2474, Val DICE Score=0.7548\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 8/15 (LR: 0.0001000): Train Loss=0.2395, Val Loss=0.3859, Val DICE Score=0.6153\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 1 epochsã€‚\n",
      "Epoch 9/15 (LR: 0.0001000): Train Loss=0.2481, Val Loss=0.4574, Val DICE Score=0.5430\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 2 epochsã€‚\n",
      "Epoch 10/15 (LR: 0.0001000): Train Loss=0.2585, Val Loss=0.4093, Val DICE Score=0.5914\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 3 epochsã€‚\n",
      "Epoch 11/15 (LR: 0.0001000): Train Loss=0.2494, Val Loss=0.3230, Val DICE Score=0.6775\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 4 epochsã€‚\n",
      "Epoch 12/15 (LR: 0.0001000): Train Loss=0.2513, Val Loss=0.4195, Val DICE Score=0.5809\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 5 epochsã€‚\n",
      "Epoch 13/15 (LR: 0.0001000): Train Loss=0.2372, Val Loss=0.2857, Val DICE Score=0.7146\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 6 epochsã€‚\n",
      "Epoch 14/15 (LR: 0.0000100): Train Loss=0.2318, Val Loss=0.3555, Val DICE Score=0.6452\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 7 epochsã€‚\n",
      "Epoch 15/15 (LR: 0.0000100): Train Loss=0.2446, Val Loss=0.3208, Val DICE Score=0.6798\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 8 epochsã€‚\n",
      "ç¸½è¨“ç·´ä»£æ•¸å·²å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# éµå®ˆè«–æ–‡è¨­ç½®çš„åƒæ•¸\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 15 # ç¸½è¨“ç·´ä»£æ•¸\n",
    "INITIAL_LR = 0.0001\n",
    "FINE_TUNE_LR = 0.00001 # æœ€ä½å­¸ç¿’ç‡ (min_lr)ï¼Œæ¨¡æ“¬ Fine-tuning éšæ®µ\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- æ¨¡å‹èˆ‡å„ªåŒ–å™¨è¨­å®š ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResDenseUNet(in_channels=3, out_channels=1).to(device)\n",
    "\n",
    "# ä½¿ç”¨è«–æ–‡è¨­ç½®çš„åˆå§‹å­¸ç¿’ç‡\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=INITIAL_LR)\n",
    "\n",
    "# --- å•Ÿå‹•è¨“ç·´ ---\n",
    "train_model_optimized_v3(model, train_loader, val_loader, optimizer, NUM_EPOCHS, device,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "489f6951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ygz08\\AppData\\Local\\Temp\\ipykernel_3428\\2979579552.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(keep_train_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸåŠ è¼‰æ¨¡å‹æ¬Šé‡ï¼Œå¾ä»¥ä¸‹è·¯å¾‘ç¹¼çºŒè¨“ç·´: D:\\Daniel\\for_git\\LDCT_git\\TSCNN\\first_stage_code\\pth\\best_stage1_unet_temp0.pth\n",
      "è¨“ç·´é–‹å§‹ï¼šBatch Size=64, åˆå§‹/ç•¶å‰ LR=0.0000100\n",
      "Epoch 1/60 (LR: 0.0000100): Train Loss=0.2439, Val Loss=0.3874, Val DICE Score=0.6137\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 2/60 (LR: 0.0000100): Train Loss=0.2448, Val Loss=0.3863, Val DICE Score=0.6148\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 3/60 (LR: 0.0000100): Train Loss=0.2445, Val Loss=0.3537, Val DICE Score=0.6470\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 4/60 (LR: 0.0000100): Train Loss=0.2399, Val Loss=0.3630, Val DICE Score=0.6378\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 1 epochsã€‚\n",
      "Epoch 5/60 (LR: 0.0000100): Train Loss=0.2339, Val Loss=0.4044, Val DICE Score=0.5963\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 2 epochsã€‚\n",
      "Epoch 6/60 (LR: 0.0000100): Train Loss=0.2270, Val Loss=0.4061, Val DICE Score=0.5948\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 3 epochsã€‚\n",
      "Epoch 7/60 (LR: 0.0000100): Train Loss=0.2306, Val Loss=0.3902, Val DICE Score=0.6104\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 4 epochsã€‚\n",
      "Epoch 8/60 (LR: 0.0000100): Train Loss=0.2373, Val Loss=0.3526, Val DICE Score=0.6479\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ä¸¦é‡ç½®æ—©åœè¨ˆæ•¸ã€‚\n",
      "Epoch 9/60 (LR: 0.0000100): Train Loss=0.2347, Val Loss=0.3731, Val DICE Score=0.6275\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 1 epochsã€‚\n",
      "Epoch 10/60 (LR: 0.0000100): Train Loss=0.2336, Val Loss=0.3838, Val DICE Score=0.6167\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 2 epochsã€‚\n",
      "Epoch 11/60 (LR: 0.0000100): Train Loss=0.2252, Val Loss=0.4041, Val DICE Score=0.5964\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 3 epochsã€‚\n",
      "Epoch 12/60 (LR: 0.0000100): Train Loss=0.2420, Val Loss=0.3752, Val DICE Score=0.6252\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 4 epochsã€‚\n",
      "Epoch 13/60 (LR: 0.0000100): Train Loss=0.2343, Val Loss=0.3849, Val DICE Score=0.6155\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 5 epochsã€‚\n",
      "Epoch 14/60 (LR: 0.0000100): Train Loss=0.2320, Val Loss=0.3719, Val DICE Score=0.6286\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 6 epochsã€‚\n",
      "Epoch 15/60 (LR: 0.0000100): Train Loss=0.2469, Val Loss=0.3629, Val DICE Score=0.6376\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 7 epochsã€‚\n",
      "Epoch 16/60 (LR: 0.0000100): Train Loss=0.2419, Val Loss=0.3631, Val DICE Score=0.6373\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 8 epochsã€‚\n",
      "Epoch 17/60 (LR: 0.0000100): Train Loss=0.2252, Val Loss=0.4085, Val DICE Score=0.5922\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 9 epochsã€‚\n",
      "Epoch 18/60 (LR: 0.0000100): Train Loss=0.2272, Val Loss=0.3735, Val DICE Score=0.6271\n",
      "æ€§èƒ½æœªæå‡ã€‚æ—©åœå·²åœæ»¯ 10 epochsã€‚\n",
      "ğŸš¨ æ—©åœè§¸ç™¼ï¼é©—è­‰æ€§èƒ½é€£çºŒ 10 epochs æœªæå‡ï¼Œä¸”å·²é”æœ€ä½ LR (2e-05)ã€‚è¨“ç·´å®Œå…¨åœæ­¢ã€‚\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 60 # ç¸½è¨“ç·´ä»£æ•¸\n",
    "INITIAL_LR = 0.00001\n",
    "FINE_TUNE_LR = 0.00002 # æœ€ä½å­¸ç¿’ç‡ (min_lr)ï¼Œæ¨¡æ“¬ Fine-tuning éšæ®µ\n",
    "\n",
    "path = r\"D:\\Daniel\\for_git\\LDCT_git\\TSCNN\\first_stage_code\\pth\\best_stage1_unet_temp0.pth\"\n",
    "train_model_optimized_v3(model, train_loader, val_loader, optimizer, NUM_EPOCHS, device, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45097b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f64150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDCT_310_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
