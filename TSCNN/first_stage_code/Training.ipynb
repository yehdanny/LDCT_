{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6811e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ygz08\\AppData\\Local\\Temp\\ipykernel_3428\\2084189178.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(LOAD_TRAIN_PATH)\n",
      "C:\\Users\\ygz08\\AppData\\Local\\Temp\\ipykernel_3428\\2084189178.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_data = torch.load(LOAD_VAL_PATH)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "LOAD_TRAIN_PATH = r\"D:\\Daniel\\for_git\\LDCT_git\\TSCNN\\first_stage_code\\TrainVal\\TRAIN.pth\"\n",
    "\n",
    "LOAD_VAL_PATH = r\"D:\\Daniel\\for_git\\LDCT_git\\TSCNN\\first_stage_code\\TrainVal\\VAL.pth\"\n",
    "\n",
    "train_data = torch.load(LOAD_TRAIN_PATH)\n",
    "val_data = torch.load(LOAD_VAL_PATH)\n",
    "\n",
    "load_train_patches = train_data['patches'] #(N_total, 3, 64, 64) #0~255\n",
    "load_train_labels = train_data['masks'] #(N_total, 1, 64, 64) #0 or 1\n",
    "\n",
    "load_val_patches = val_data['patches'] #(N_total, 3, 64, 64) #0~255\n",
    "load_val_labels = val_data['masks'] #(N_total, 1, 64, 64) #0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd2342a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train æœ‰ 160 å€‹æ¨£æœ¬ï¼ŒVal æœ‰ 30 å€‹æ¨£æœ¬ã€‚\n",
      "âœ… æ‰€æœ‰ Tensor çš„ç¶­åº¦èˆ‡å‹æ…‹æª¢æŸ¥é€šéï¼\n"
     ]
    }
   ],
   "source": [
    "if train_data and val_data :\n",
    "    # æª¢æŸ¥è¨“ç·´ Patches ç¶­åº¦\n",
    "    # ç¢ºä¿æ˜¯å››å€‹ç¶­åº¦ï¼Œä¸” Channel=3, H=64, W=64ã€‚N_total ä¸å›ºå®šï¼Œç”¨ -1 æª¢æŸ¥\n",
    "    assert load_train_patches.dim() == 4, \\\n",
    "        f\"è¨“ç·´ Patches ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_train_patches.dim()} ç¶­.\"\n",
    "    assert load_train_patches.shape[1:] == (3, 64, 64), \\\n",
    "        f\"è¨“ç·´ Patches Shape éŒ¯èª¤! é æœŸ (N, 3, 64, 64), å¯¦éš› {load_train_patches.shape}.\"\n",
    "\n",
    "    # æª¢æŸ¥è¨“ç·´ Labels (Masks) ç¶­åº¦\n",
    "    # ç¢ºä¿æ˜¯å››å€‹ç¶­åº¦ï¼Œä¸” Channel=1, H=64, W=64\n",
    "    assert load_train_labels.dim() == 4, \\\n",
    "        f\"è¨“ç·´ Labels ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_train_labels.dim()} ç¶­.\"\n",
    "    assert load_train_labels.shape[1:] == (1, 64, 64), \\\n",
    "        f\"è¨“ç·´ Labels Shape éŒ¯èª¤! é æœŸ (N, 1, 64, 64), å¯¦éš› {load_train_labels.shape}.\"\n",
    "\n",
    "    # æª¢æŸ¥è¨“ç·´è³‡æ–™çš„æ‰¹æ¬¡å¤§å°æ˜¯å¦ä¸€è‡´\n",
    "    assert load_train_patches.shape[0] == load_train_labels.shape[0], \\\n",
    "        f\"è¨“ç·´è³‡æ–™æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´! Patches: {load_train_patches.shape[0]}, Labels: {load_train_labels.shape[0]}.\"\n",
    "        \n",
    "    # å»ºè­°çš„è³‡æ–™å‹æ…‹æª¢æŸ¥\n",
    "    # Patches (è¼¸å…¥å½±åƒ) é€šå¸¸æ˜¯ float\n",
    "    # Labels (é®ç½©) é€šå¸¸æ˜¯ float æˆ– long (å¦‚æœæ˜¯é¡åˆ¥ç´¢å¼•)\n",
    "    assert load_train_patches.dtype == torch.float32, \\\n",
    "        f\"è¨“ç·´ Patches è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32, å¯¦éš› {load_train_patches.dtype}.\"\n",
    "    assert load_train_labels.dtype in [torch.float32, torch.long], \\\n",
    "        f\"è¨“ç·´ Labels è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32 æˆ– torch.long, å¯¦éš› {load_train_labels.dtype}.\"\n",
    "\n",
    "    # --- ğŸ¯ æª¢æŸ¥é©—è­‰è³‡æ–™ (VAL Data Assertions) ---\n",
    "\n",
    "    # æª¢æŸ¥é©—è­‰ Patches ç¶­åº¦\n",
    "    assert load_val_patches.dim() == 4, \\\n",
    "        f\"é©—è­‰ Patches ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_val_patches.dim()} ç¶­.\"\n",
    "    assert load_val_patches.shape[1:] == (3, 64, 64), \\\n",
    "        f\"é©—è­‰ Patches Shape éŒ¯èª¤! é æœŸ (N, 3, 64, 64), å¯¦éš› {load_val_patches.shape}.\"\n",
    "\n",
    "    # æª¢æŸ¥é©—è­‰ Labels (Masks) ç¶­åº¦\n",
    "    assert load_val_labels.dim() == 4, \\\n",
    "        f\"é©—è­‰ Labels ç¶­åº¦éŒ¯èª¤! é æœŸ 4 ç¶­, å¯¦éš› {load_val_labels.dim()} ç¶­.\"\n",
    "    assert load_val_labels.shape[1:] == (1, 64, 64), \\\n",
    "        f\"é©—è­‰ Labels Shape éŒ¯èª¤! é æœŸ (N, 1, 64, 64), å¯¦éš› {load_val_labels.shape}.\"\n",
    "        \n",
    "    # æª¢æŸ¥é©—è­‰è³‡æ–™çš„æ‰¹æ¬¡å¤§å°æ˜¯å¦ä¸€è‡´\n",
    "    assert load_val_patches.shape[0] == load_val_labels.shape[0], \\\n",
    "        f\"é©—è­‰è³‡æ–™æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´! Patches: {load_val_patches.shape[0]}, Labels: {load_val_labels.shape[0]}.\"\n",
    "\n",
    "    # å»ºè­°çš„è³‡æ–™å‹æ…‹æª¢æŸ¥\n",
    "    assert load_val_patches.dtype == torch.float32, \\\n",
    "        f\"é©—è­‰ Patches è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32, å¯¦éš› {load_val_patches.dtype}.\"\n",
    "    assert load_val_labels.dtype in [torch.float32, torch.long], \\\n",
    "        f\"é©—è­‰ Labels è³‡æ–™å‹æ…‹éŒ¯èª¤! é æœŸ torch.float32 æˆ– torch.long, å¯¦éš› {load_val_labels.dtype}.\"\n",
    "\n",
    "    print(\"âœ… Train æœ‰ {} å€‹æ¨£æœ¬ï¼ŒVal æœ‰ {} å€‹æ¨£æœ¬ã€‚\".format(load_train_patches.shape[0], load_val_patches.shape[0]))\n",
    "    print(\"âœ… æ‰€æœ‰ Tensor çš„ç¶­åº¦èˆ‡å‹æ…‹æª¢æŸ¥é€šéï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0012b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´è³‡æ–™é›†å¤§å°: 160\n",
      "è¨“ç·´æ‰¹æ¬¡æ•¸é‡: 5\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1. ç¢ºä¿æ•¸æ“šé¡å‹å’Œç¯„åœæ­£ç¢º\n",
    "# patches (å½±åƒ) é€šå¸¸éœ€è¦æ˜¯ float é¡å‹ï¼Œlabels (æ©ç¢¼) ä¹Ÿæ˜¯ float (å› ç‚º sigmoid è¼¸å‡º)\n",
    "# ä¸¦ä¸”å°‡ 0~255 çš„ patches æ­¸ä¸€åŒ–åˆ° 0~1 ç¯„åœ (å¦‚æœå°šæœªæ­¸ä¸€åŒ–)\n",
    "\n",
    "# å‡è¨­åŸå§‹ patches æ˜¯ 0~255 çš„ uint8 æˆ– float é¡å‹\n",
    "# é€™è£¡å°‡å…¶è½‰ç‚º float ä¸¦æ­¸ä¸€åŒ– (å¦‚æœæ‚¨çš„é è™•ç†é‚„æ²’åšé€™æ­¥)\n",
    "# å¦‚æœæ‚¨çš„æ•¸æ“šå·²ç¶“æ˜¯æ­¸ä¸€åŒ–çš„ floatï¼Œå¯ä»¥è·³éé€™ä¸€æ­¥çš„é™¤ä»¥ 255\n",
    "load_train_patches = load_train_patches.float() / 255.0\n",
    "load_val_patches = load_val_patches.float() / 255.0\n",
    "\n",
    "load_train_labels = load_train_labels.float()\n",
    "load_val_labels = load_val_labels.float()\n",
    "\n",
    "# 2. å‰µå»º TensorDataset\n",
    "train_dataset = TensorDataset(load_train_patches, load_train_labels)\n",
    "val_dataset = TensorDataset(load_val_patches, load_val_labels)\n",
    "\n",
    "# 3. å‰µå»º DataLoader\n",
    "BATCH_SIZE = 32 # å¯ä»¥æ ¹æ“šä½ çš„ GPU è¨˜æ†¶é«”èª¿æ•´\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"è¨“ç·´è³‡æ–™é›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"è¨“ç·´æ‰¹æ¬¡æ•¸é‡: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68701f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Dice Loss å‡½æ•¸\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    \"\"\"è¨ˆç®— Dice Loss\"\"\"\n",
    "    # å±•å¹³å¼µé‡\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    \n",
    "    # è¨ˆç®—äº¤é›†å’Œç¸½å’Œ\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    \n",
    "    return 1 - dice\n",
    "\n",
    "# 2. ç¸½é«”æå¤±å‡½æ•¸ (BCE + Dice)\n",
    "def combined_loss(pred, target, alpha=0.5):\n",
    "    \"\"\"çµåˆ Binary Cross-Entropy Loss å’Œ Dice Loss\"\"\"\n",
    "    # BCE Loss (é©ç”¨æ–¼ Sigmoid è¼¸å‡ºçš„äºŒå…ƒåˆ†å‰²)\n",
    "    bce = F.binary_cross_entropy(pred, target)\n",
    "    \n",
    "    # Dice Loss\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    # çµåˆæå¤± (alpha æ¬Šé‡å¯èª¿)\n",
    "    return alpha * bce + (1 - alpha) * dice\n",
    "def dice_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"è¨ˆç®— Dice ä¿‚æ•¸ (Dice Coefficient)\"\"\"\n",
    "    # ç¢ºä¿è¼¸å…¥æ˜¯äºŒå…ƒ (0 æˆ– 1)\n",
    "    # æˆ‘å€‘å°‡ Sigmoid è¼¸å‡º (0-1) è½‰æ›ç‚ºäºŒå…ƒé æ¸¬ (0/1)ï¼Œé–¾å€¼è¨­ç‚º 0.5\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    # å±•å¹³å¼µé‡\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    \n",
    "    # è¨ˆç®—äº¤é›† (Intersection) å’Œç¸½å’Œ (Union)\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    \n",
    "    return dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07933311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 1. åŸºç¤å–®å…ƒ: DenseBlock (DB) ---\n",
    "class DenseBlock(nn.Module):\n",
    "    # growth_rate å°±æ˜¯è«–æ–‡ä¸­çš„ n (8, 16, 32, ...)\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        \n",
    "        # 1. ç¬¬ä¸€å€‹ Conv: BN -> ReLU -> Conv\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels), # æœŸæœ›è¼¸å…¥é€šé“æ•¸\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False),\n",
    "            nn.Dropout(dropout_rate) \n",
    "        )\n",
    "        \n",
    "        # 2. ç¬¬äºŒå€‹ Conv: BN -> ReLU -> Conv\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(growth_rate), # è¼¸å…¥ä¾†è‡ªä¸Šä¸€å€‹ conv1 çš„è¼¸å‡º\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(growth_rate, growth_rate, kernel_size=3, padding=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x) \n",
    "        x2 = self.conv2(x1)\n",
    "        \n",
    "        # è¼¸å‡ºæ˜¯ (è¼¸å…¥ x) å’Œ (æœ€çµ‚å·ç©è¼¸å‡º x2) çš„ä¸²è¯\n",
    "        return torch.cat([x, x2], dim=1) \n",
    "\n",
    "\n",
    "# --- 2. å †ç–Šå–®å…ƒ: Residual Dense Block (RDB) ---\n",
    "class ResDenseBlock(nn.Module):\n",
    "    # RDB çš„ in_channels å¿…é ˆç­‰æ–¼å®ƒçš„è¼¸å‡ºé€šé“æ•¸ (ç‚ºæ»¿è¶³ U-Net çš„è·³èºé€£æ¥è¦æ±‚)\n",
    "    def __init__(self, in_channels, n_feature, n_layers=6, dropout_rate=0.2):\n",
    "        super(ResDenseBlock, self).__init__()\n",
    "        \n",
    "        self.n_feature = n_feature \n",
    "        layers = []\n",
    "        current_channels = in_channels # å¾ RDB çš„è¼¸å…¥é€šé“é–‹å§‹\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            # æ¯å±¤ DB çš„è¼¸å…¥æ˜¯å‰ä¸€å±¤çš„ç¸½è¼¸å‡º\n",
    "            db = DenseBlock(current_channels, self.n_feature, dropout_rate)\n",
    "            layers.append(db)\n",
    "            # é€šé“æ•¸å¢åŠ  n_feature\n",
    "            current_channels += self.n_feature \n",
    "\n",
    "        self.dense_blocks = nn.Sequential(*layers)\n",
    "        \n",
    "        # 1x1 å·ç©éæ¸¡å±¤ (Transition Layer): \n",
    "        # å°‡ç¸½è¼¸å‡ºé€šé“æ•¸ (current_channels) è½‰æ›å› RDB æœŸæœ›çš„è¼¸å‡ºé€šé“ (in_channels)ï¼Œä»¥é€²è¡Œæ®˜å·®é€£æ¥\n",
    "        # é€™æ˜¯æ»¿è¶³ RDB è¼¸å‡ºé€šé“ = è¼¸å…¥é€šé“çš„é—œéµï¼\n",
    "        self.transition_conv = nn.Conv2d(current_channels, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_output = self.dense_blocks(x)\n",
    "        processed_output = self.transition_conv(dense_output)\n",
    "        \n",
    "        # æ®˜å·®é€£æ¥ï¼šè¼¸å‡º = å£“ç¸®å¾Œçš„å¯†é›†è¼¸å‡º + åŸå§‹è¼¸å…¥\n",
    "        return processed_output + x\n",
    "\n",
    "\n",
    "# --- 3. æ•´é«”æ¶æ§‹ï¼šResDense UNet (ä¿®æ­£ç‰ˆ 3) ---\n",
    "class ResDenseUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(ResDenseUNet, self).__init__()\n",
    "        \n",
    "        # è«–æ–‡åƒæ•¸ n: [8, 16, 32, 32, 16, 8]\n",
    "        n_features = [8, 16, 32, 32, 16, 8] \n",
    "        \n",
    "        # --- é€šç”¨å±¤ ---\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        def create_upconv(in_c, out_c):\n",
    "            # ä½¿ç”¨ output_padding=1 ç¢ºä¿æ­£ç¢ºçš„å°ºå¯¸åŠ å€ (ä¾‹å¦‚ 8->16, 16->32)\n",
    "            return nn.ConvTranspose2d(in_c, out_c, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # --- Encoder (ç¸®æ¸›è·¯å¾‘) ---\n",
    "        \n",
    "        # 1. åˆå§‹ Conv (3 -> 8)\n",
    "        self.init_conv = nn.Conv2d(in_channels, n_features[0], kernel_size=1) \n",
    "        self.rdb1 = ResDenseBlock(in_channels=n_features[0], n_feature=n_features[0]) # Output C=8\n",
    "\n",
    "        # 2. é€šé“èª¿æ•´ (Max Pool å¾Œ): 8 -> 16\n",
    "        self.trans_conv1 = nn.Conv2d(n_features[0], n_features[1], kernel_size=1) \n",
    "        self.rdb2 = ResDenseBlock(in_channels=n_features[1], n_feature=n_features[1]) # Output C=16\n",
    "\n",
    "        # 3. é€šé“èª¿æ•´ (Max Pool å¾Œ): 16 -> 32\n",
    "        self.trans_conv2 = nn.Conv2d(n_features[1], n_features[2], kernel_size=1) \n",
    "        self.rdb3 = ResDenseBlock(in_channels=n_features[2], n_feature=n_features[2]) # Output C=32 (Bottleneck Input)\n",
    "\n",
    "        # --- Decoder (æ“´å¢è·¯å¾‘) ---\n",
    "        \n",
    "        # 4. Stage 4: 8x8 -> 16x16\n",
    "        # upconv4 è¼¸å…¥=Bottleneck è¼¸å‡º(32), è¼¸å‡º=n_features[3]=32 (ç”¨æ–¼ Concatenate)\n",
    "        self.upconv4 = create_upconv(n_features[2], n_features[3]) \n",
    "        # rdb4 è¼¸å…¥: upconv(32) + skip(32) = 64\n",
    "        self.rdb4 = ResDenseBlock(in_channels=n_features[3] + n_features[2], n_feature=n_features[3]) # Output C=64\n",
    "        \n",
    "        # 5. Stage 5: 16x16 -> 32x32\n",
    "        # upconv5 è¼¸å…¥=rdb4_out(64), è¼¸å‡º=n_features[4]=16\n",
    "        self.upconv5 = create_upconv(n_features[3] + n_features[2], n_features[4])\n",
    "        # rdb5 è¼¸å…¥: upconv(16) + skip(16) = 32\n",
    "        self.rdb5 = ResDenseBlock(in_channels=n_features[4] + n_features[1], n_feature=n_features[4]) # Output C=32\n",
    "\n",
    "        # 6. Stage 6: 32x32 -> 64x64\n",
    "        # upconv6 è¼¸å…¥=rdb5_out(32), è¼¸å‡º=n_features[5]=8\n",
    "        self.upconv6 = create_upconv(n_features[4] + n_features[1], n_features[5])\n",
    "        # rdb6 è¼¸å…¥: upconv(8) + skip(8) = 16\n",
    "        self.rdb6 = ResDenseBlock(in_channels=n_features[5] + n_features[0], n_feature=n_features[5]) # Output C=16\n",
    "\n",
    "        # --- æœ€çµ‚è¼¸å‡ºå±¤ (16 -> 1) ---\n",
    "        self.final_conv = nn.Conv2d(n_features[5] + n_features[0], out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # --- æ‰“å° Shape ä»¥ä¾›æª¢æŸ¥ ---\n",
    "        # print(f\"Input: {x.shape}\")\n",
    "        \n",
    "        # 1. Initial Conv & RDB1\n",
    "        x = self.init_conv(x) \n",
    "        rdb1_out = self.rdb1(x) # Skip 1 (C=8, 64x64)\n",
    "        \n",
    "        # 2. Stage 2\n",
    "        p1 = self.pool(rdb1_out) \n",
    "        t1 = self.trans_conv1(p1) # C: 8 -> 16\n",
    "        rdb2_out = self.rdb2(t1) # Skip 2 (C=16, 32x32)\n",
    "        \n",
    "        # 3. Stage 3\n",
    "        p2 = self.pool(rdb2_out)\n",
    "        t2 = self.trans_conv2(p2) # C: 16 -> 32\n",
    "        rdb3_out = self.rdb3(t2) # Skip 3 (C=32, 16x16)\n",
    "        \n",
    "        # 4. Bottleneck\n",
    "        p3 = self.pool(rdb3_out) # Bottleneck (C=32, 8x8)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        \n",
    "        # 5. Stage 4: 8x8 -> 16x16\n",
    "        up4 = self.upconv4(p3) # C=32\n",
    "        cat4 = torch.cat([up4, rdb3_out], dim=1) # C=64\n",
    "        rdb4_out = self.rdb4(cat4) # C=64\n",
    "        \n",
    "        # 6. Stage 5: 16x16 -> 32x32\n",
    "        up5 = self.upconv5(rdb4_out) # C=16\n",
    "        cat5 = torch.cat([up5, rdb2_out], dim=1) # C=32\n",
    "        rdb5_out = self.rdb5(cat5) # C=32\n",
    "\n",
    "        # 7. Stage 6: 32x32 -> 64x64\n",
    "        up6 = self.upconv6(rdb5_out) # C=8\n",
    "        cat6 = torch.cat([up6, rdb1_out], dim=1) # C=16\n",
    "        rdb6_out = self.rdb6(cat6) # C=16\n",
    "\n",
    "        # 8. Output\n",
    "        return self.sigmoid(self.final_conv(rdb6_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# éµå®ˆè«–æ–‡è¨­ç½®çš„åƒæ•¸\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 15 # ç¸½è¨“ç·´ä»£æ•¸\n",
    "INITIAL_LR = 0.0001\n",
    "FINE_TUNE_LR = 0.00001 # é›–ç„¶è«–æ–‡æœªæ˜ç¢ºèªªæ˜ä½•æ™‚é€²è¡Œ Fine-tuningï¼Œä½†æˆ‘å€‘å°‡ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦ä¾†æ¨¡æ“¬\n",
    "\n",
    "def train_model_optimized(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
    "    \n",
    "    # åˆå§‹åŒ–å­¸ç¿’ç‡èª¿åº¦å™¨ï¼Œæ¨¡æ“¬å­¸ç¿’ç‡ä¸‹é™æ©Ÿåˆ¶ (ä¾‹å¦‚åœ¨æ€§èƒ½åœæ»¯æ™‚é™ä½ LR)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    best_dice = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop_patience = 5 # è«–æ–‡è¦å®šçš„æ—©åœå¯¬é™æœŸ\n",
    "    \n",
    "    print(f\"è¨“ç·´é–‹å§‹ï¼šBatch Size={BATCH_SIZE}, LR={optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ------------------ è¨“ç·´éšæ®µ ------------------\n",
    "        model.train() \n",
    "        train_loss_sum = 0\n",
    "        \n",
    "        for inputs, masks in train_loader:\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = combined_loss(outputs, masks) \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += loss.item() * inputs.size(0)\n",
    "            \n",
    "        train_loss_avg = train_loss_sum / len(train_loader.dataset)\n",
    "        \n",
    "        # ------------------ é©—è­‰éšæ®µ ------------------\n",
    "        model.eval() \n",
    "        val_loss_sum = 0\n",
    "        val_dice_sum = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, masks in val_loader:\n",
    "                inputs, masks = inputs.to(device), masks.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = combined_loss(outputs, masks)\n",
    "                val_loss_sum += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # è¨ˆç®— Dice ä¿‚æ•¸\n",
    "                val_dice_sum += dice_score(outputs, masks) * inputs.size(0)\n",
    "                \n",
    "        val_loss_avg = val_loss_sum / len(val_loader.dataset)\n",
    "        val_dice_avg = val_dice_sum / len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss_avg:.4f}, \"\n",
    "              f\"Val Loss={val_loss_avg:.4f}, Val DICE Score={val_dice_avg:.4f}\")\n",
    "        \n",
    "        # ------------------ æ—©åœèˆ‡å­¸ç¿’ç‡èª¿åº¦ ------------------\n",
    "        \n",
    "        # æ ¹æ“š Dice Score èª¿æ•´å­¸ç¿’ç‡\n",
    "        scheduler.step(val_dice_avg)\n",
    "        \n",
    "        # æª¢æŸ¥ Dice Score æ˜¯å¦æå‡\n",
    "        if val_dice_avg > best_dice:\n",
    "            best_dice = val_dice_avg\n",
    "            epochs_no_improve = 0\n",
    "            # ğŸ’¡ æœ€ä½³æ¨¡å‹æ¬Šé‡ä¿å­˜ (Checkpointing)\n",
    "            torch.save(model.state_dict(), 'pth/best_stage1_unet.pth')\n",
    "            print(\"ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ {epochs_no_improve} epochsã€‚\")\n",
    "            \n",
    "        # åŸ·è¡Œæ—©åœç­–ç•¥\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"ğŸš¨ æ—©åœè§¸ç™¼ï¼é©—è­‰æ€§èƒ½é€£çºŒ {early_stop_patience} epochs æœªæå‡ã€‚\")\n",
    "            # è¨­ç½®æ–°çš„å­¸ç¿’ç‡ï¼Œæ¨¡æ“¬è«–æ–‡ä¸­çš„ Fine-tuning æ­¥é©Ÿ\n",
    "            for param_group in optimizer.param_groups:\n",
    "                if param_group['lr'] > FINE_TUNE_LR:\n",
    "                     param_group['lr'] = FINE_TUNE_LR\n",
    "                     print(f\"âœ… å­¸ç¿’ç‡å·²é™è‡³ Fine-tuning LR: {FINE_TUNE_LR}\")\n",
    "                     epochs_no_improve = 0 # é‡ç½®è¨ˆæ•¸å™¨ï¼Œè®“æ¨¡å‹æœ‰ 5 æ¬¡æ©Ÿæœƒé©æ‡‰æ–° LR\n",
    "                     early_stop_patience = 5 # ä¿æŒ 5 æ¬¡å¯¬é™æœŸ\n",
    "                else:\n",
    "                    # å¦‚æœå·²ç¶“æ˜¯æœ€ä½ LRï¼Œå‰‡å®Œå…¨åœæ­¢è¨“ç·´\n",
    "                    print(\"âš ï¸ Fine-tuning LR å·²æ‡‰ç”¨ï¼Œä½†ä»æœªæ”¹å–„ï¼Œè¨“ç·´å®Œå…¨åœæ­¢ã€‚\")\n",
    "                    return \n",
    "\n",
    "    print(\"ç¸½è¨“ç·´ä»£æ•¸å·²å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a53e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´é–‹å§‹ï¼šBatch Size=64, LR=0.0001\n",
      "Epoch 1/15: Train Loss=0.8882, Val Loss=0.6558, Val DICE Score=0.5280\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚\n",
      "Epoch 2/15: Train Loss=0.8703, Val Loss=0.6546, Val DICE Score=0.5638\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚\n",
      "Epoch 3/15: Train Loss=0.8533, Val Loss=0.6528, Val DICE Score=0.6054\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚\n",
      "Epoch 4/15: Train Loss=0.8367, Val Loss=0.6503, Val DICE Score=0.6487\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚\n",
      "Epoch 5/15: Train Loss=0.8193, Val Loss=0.6471, Val DICE Score=0.6833\n",
      "ğŸš€ Dice Score æå‡ï¼ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚\n",
      "Epoch 6/15: Train Loss=0.8015, Val Loss=0.6433, Val DICE Score=0.6389\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 1 epochsã€‚\n",
      "Epoch 7/15: Train Loss=0.7829, Val Loss=0.6396, Val DICE Score=0.5855\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 2 epochsã€‚\n",
      "Epoch 8/15: Train Loss=0.7632, Val Loss=0.6368, Val DICE Score=0.4424\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 3 epochsã€‚\n",
      "Epoch 9/15: Train Loss=0.7426, Val Loss=0.6360, Val DICE Score=0.1957\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 4 epochsã€‚\n",
      "Epoch 10/15: Train Loss=0.7213, Val Loss=0.6383, Val DICE Score=0.0242\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 5 epochsã€‚\n",
      "ğŸš¨ æ—©åœè§¸ç™¼ï¼é©—è­‰æ€§èƒ½é€£çºŒ 5 epochs æœªæå‡ã€‚\n",
      "âœ… å­¸ç¿’ç‡å·²é™è‡³ Fine-tuning LR: 1e-05\n",
      "Epoch 11/15: Train Loss=0.7042, Val Loss=0.6396, Val DICE Score=0.0059\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 1 epochsã€‚\n",
      "Epoch 12/15: Train Loss=0.7029, Val Loss=0.6413, Val DICE Score=0.0032\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 2 epochsã€‚\n",
      "Epoch 13/15: Train Loss=0.7026, Val Loss=0.6448, Val DICE Score=0.0026\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 3 epochsã€‚\n",
      "Epoch 14/15: Train Loss=0.7022, Val Loss=0.6463, Val DICE Score=0.0029\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 4 epochsã€‚\n",
      "Epoch 15/15: Train Loss=0.7020, Val Loss=0.6486, Val DICE Score=0.0029\n",
      "æ€§èƒ½æœªæå‡ã€‚å·²åœæ»¯ 5 epochsã€‚\n",
      "ğŸš¨ æ—©åœè§¸ç™¼ï¼é©—è­‰æ€§èƒ½é€£çºŒ 5 epochs æœªæå‡ã€‚\n",
      "âš ï¸ Fine-tuning LR å·²æ‡‰ç”¨ï¼Œä½†ä»æœªæ”¹å–„ï¼Œè¨“ç·´å®Œå…¨åœæ­¢ã€‚\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 64 # éµå®ˆè«–æ–‡è¨­ç½®\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- æ¨¡å‹èˆ‡å„ªåŒ–å™¨è¨­å®š ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResDenseUNet(in_channels=3, out_channels=1).to(device)\n",
    "\n",
    "# ä½¿ç”¨è«–æ–‡è¨­ç½®çš„åˆå§‹å­¸ç¿’ç‡\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=INITIAL_LR)\n",
    "\n",
    "# --- å•Ÿå‹•è¨“ç·´ ---\n",
    "train_model_optimized(model, train_loader, val_loader, optimizer, NUM_EPOCHS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f6951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDCT_310_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
